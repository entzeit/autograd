## Autogradient Matrix Implementation for Neural Networks

This project is a practical approach designed for educational purposes. It is highly inspired by [Micrograd](https://github.com/karpathy/micrograd), but instead of working with atomic values, it implements auto gradient descent on **matrices**.

This project is grounded in the mathematical concept of implementing neural networks using matrix operations. It provides a comprehensive suite of matrix operations essential for neural network functionality. These include:
- **Fundamental atomic functions** such as:
  - Addition, Multiplication, Dot product
- **More abstract operations** like:
  - Sigmoid, ReLU, LeakyReLU, activation function & other necessary operations for neural network functionality
- **Loss functions**:
  - MSE, MAE

#### Future TODOs
- Implement batch_size
- Implement no_grad
- Fix BinaryCrossEntropy and Log
- Improve Performance

### License
MIT
