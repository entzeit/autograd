# autograd

Autogradient Matrix implementation for Neural Networks. This is a practical approach for educational purposes.
It's highly inspired by [Micrograd](https://github.com/karpathy/micrograd),but implements auto gradient descent on matrices instead of atomic values.
This project is grounded in the mathematical concept of implementing neural networks using matrix operations. It provides a comprehensive suite of matrix operations essential for neural network functionality. These include fundamental atomic functions such as addition, multiplication, and dot product, as well as more abstract operations like the sigmoid activation function and others.

## TODOs

- [ ] Implement batch_size
- [ ] Implement no_grad
- [ ] Fix BinaryCrossEntropy and Log
- [ ] Improve Performance

### License

MIT
